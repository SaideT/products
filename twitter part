
library(twitteR)
library(rtweet)
library(httr)
library(curl)
library(jsonlite)
library(dplyr)
library(tidytext)
library(stringr)
library(purrr)
library(ggplot2)
library(tidyr)
library(wordcloud)

# Get the api keys from twitter and setup twitter oauth

key <- "key1"
secret <- "secret1"
access_token <- "access_token1"
access_secret <- "access_secret1"
setup_twitter_oauth(key, secret, access_token, access_secret)

# Search twitter and save them to data frames

tw1 = searchTwitter('natural shampoo', n = 1000, lang = "en")
d1 = twListToDF(tw1)
tw2 = searchTwitter('natural conditioner', n = 1000, lang = "en")
d2 = twListToDF(tw2)

# 1. Top word pairs in all tweets with “natural shampoo/conditioner” ---------------------------------------------------------

# Data clean for d1(natural shampoo), then save them in a bigram (word pair) form, then remove the stopwords as well as 
# the obvious words of "natural" and "shampoo".

d1_text <- d1 %>% select(text) %>% distinct() %>% as.matrix()
d1_tidy <- d1_text %>% 
  str_replace_all("http.+", "") %>% 
  str_remove_all("^RT") %>% 
  str_remove_all("&amp") %>% 
  str_remove_all("#.+") %>%
  as_tibble()
d1_tidy2 <- d1_tidy %>% 
  unnest_tokens(bigram, value, token = "ngrams", n = 2)
d1_tidy3 <- d1_tidy2 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
d1_tidy4 <- d1_tidy3 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% c("natural", "shampoo")) %>% 
  filter(!word2 %in% c("natural", "shampoo"))
d1_tidy5 <- d1_tidy4 %>% unite(word_pair, word1, word2, sep = " ")

# make the plot of top 20 word pairs

d1_tidy5 %>% 
  count(word_pair, sort = TRUE) %>% 
  .[1:20, ] %>% 
  mutate(word_pair = reorder(word_pair, n)) %>% 
  ggplot(aes(word_pair,n)) + 
  geom_bar(stat = 'identity') +
  coord_flip() +
  ylab("Counts") + xlab("Popular Word Pairs") + ggtitle("Topics about 'Natural Shampoo' in Twitter")
  
# Data clean for d2(natural conditioner), then save them in a bigram (word pair) form, then remove the stopwords as well as  
# the obvious words of"natural" and "conditioner".

d2_text <- d2 %>% select(text) %>% distinct() %>% as.matrix()
d2_tidy <- d2_text %>% 
  str_replace_all("http.+", "") %>% 
  str_remove_all("^RT") %>% 
  str_remove_all("&amp") %>% 
  str_remove_all("#.+") %>%
  as_tibble()
d2_tidy2 <- d2_tidy %>% 
  unnest_tokens(bigram, value, token = "ngrams", n = 2)
d2_tidy3 <- d2_tidy2 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
d2_tidy4 <- d2_tidy3 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% c("natural", "conditioner")) %>% 
  filter(!word2 %in% c("natural", "conditioner"))
d2_tidy5 <- d2_tidy4 %>% unite(word_pair, word1, word2, sep = " ") %>% 
  count(word_pair, sort = TRUE)

# Make the plot of top 20 word pairs

d2_tidy5[1:20, ] %>% 
  mutate(word_pair = reorder(word_pair, n)) %>% 
  ggplot(aes(word_pair,n)) + 
  geom_bar(stat = 'identity') +
  coord_flip() +
  ylab("Counts") + xlab("Popular Word Pairs") + ggtitle("Topics about 'Natural conditioner' in Twitter")
  
# 2. Analysis for the top favorite or reweeted tweets with "natural shampoo/conditioner" ---------------------------------------------------------

# Select the top 50 "natural shampoo"vtweets with highest favoriteCount and retweetCount.
# clean the text, extract word pairs, remove stop words and make the word pair cloud.

d1_top <- d1 %>%
  select(text, favoriteCount, created, screenName:retweeted) 
d1_top_favorite <- d1_top %>% 
  distinct(text, favoriteCount) %>% 
  arrange(desc(favoriteCount)) %>% 
  .[1:50, ]
d1_top_retweet <- d1_top %>% 
  distinct(text, retweetCount) %>% 
  arrange(desc(retweetCount)) %>% 
  .[1:50, ]
d1_top2 <- d1_top_favorite %>% 
  full_join(d1_top_retweet)
d1_top_text <- d1_top2 %>% select(text) %>% distinct() %>% as.matrix()
d1_top_tidy <- d1_top_text %>% 
  as.matrix() %>% 
  str_replace_all("http.+", "") %>% 
  str_remove_all("^RT") %>% 
  str_remove_all("&amp") %>%
  str_remove_all("#.+") %>%
  as_tibble() 
d1_top_tidy2 <- d1_top_tidy %>% 
  unnest_tokens(bigram, value, token = "ngrams", n = 2)
d1_top_tidy3 <- d1_top_tidy2 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
d1_top_tidy4 <- d1_top_tidy3 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% c("natural", "shampoo")) %>% 
  filter(!word2 %in% c("natural", "shampoo"))
d1_top_tidy5 <- d1_top_tidy4 %>% unite(word_pair, word1, word2, sep = " ")
d1_top_summary <- d1_top_tidy5 %>% 
  count(word_pair, sort = TRUE) 
d1_top_summary2 <- d1_top_summary %>% filter(str_detect(word_pair, "[0-9]") == FALSE)
d1_top_summary2[1:20, ] %>% with(wordcloud(word_pair, n, max.words = 20, scale = c(3, .2), colors = "purple"))

# Select the top 50 "natural conditioner"vtweets with highest favoriteCount and retweetCount.
# clean the text, extract word pairs, remove stop words and make the word pair cloud.

d2_top <- d2 %>%
  select(text, favoriteCount, created, screenName:retweeted) 
d2_top_favorite <- d2_top %>% 
  distinct(text, favoriteCount) %>% 
  arrange(desc(favoriteCount)) %>% 
  .[1:50, ]
d2_top_retweet <- d2_top %>% 
  distinct(text, retweetCount) %>% 
  arrange(desc(retweetCount)) %>% 
  .[1:50, ]
d2_top2 <- d2_top_favorite %>% 
  full_join(d2_top_retweet)
d2_top_text <- d2_top2 %>% select(text) %>% distinct() %>% as.matrix()
d2_top_tidy <- d2_top_text %>% 
  as.matrix() %>% 
  str_replace_all("http.+", "") %>% 
  str_remove_all("^RT") %>% 
  str_remove_all("&amp") %>%
  str_remove_all("#.+") %>%
  as_tibble() 
d2_top_tidy2 <- d2_top_tidy %>% 
  unnest_tokens(bigram, value, token = "ngrams", n = 2)
d2_top_tidy3 <- d2_top_tidy2 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
d2_top_tidy4 <- d2_top_tidy3 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% c("natural", "conditioner")) %>% 
  filter(!word2 %in% c("natural", "conditioner"))
d2_top_tidy5 <- d2_top_tidy4 %>% unite(word_pair, word1, word2, sep = " ")
d2_top_summary <- d2_top_tidy5 %>% 
  count(word_pair, sort = TRUE) 
d2_top_summary2 <- d2_top_summary %>% filter(str_detect(word_pair, "[0-9]") == FALSE)
d2_top_summary2[1:10, ] %>% with(wordcloud(word_pair, n, max.words = 10, scale = c(3, .9), colors = "green"))
  
# 3. Semtiment analysis for all tweets related with "shampoo/conditioner"  --------------------------------------------------------------

# Label the natural related tweets with "natural"
d1_tidy7 <- d1_tidy %>% 
  unnest_tokens(word, value) %>% 
  mutate(group = "shampoo_natural ") %>% 
  mutate(type = "natural") %>% 
  mutate(category = "shampoo")
d2_tidy7 <- d2_tidy %>% 
  unnest_tokens(word, value) %>% 
  mutate(group = "conditioner_natural ") %>% 
  mutate(type = "natural") %>% 
  mutate(category = "conditioner")
  
# Now we get tweets with general shampoo/conditioner (not limited to natural)

tw3 = searchTwitter('shampoo', n = 1000, lang = "en")
d3 = twListToDF(tw3)
d3_text <- d3 %>% select(text) %>% distinct() %>% as.matrix()
d3_tidy <- d3_text %>% 
  str_replace_all("http.+", "") %>% 
  str_remove_all("^RT") %>% 
  str_remove_all("&amp") %>%
  str_remove_all("#.+") %>%
  as_tibble() %>% 
  unnest_tokens(word, value) %>% 
  mutate(group = "shampoo_all") %>% 
  mutate(type = "all") %>% 
  mutate(category = "shampoo")

tw4 = searchTwitter('conditioner', n = 1000, lang = "en")
d4 = twListToDF(tw4)
d4_text <- d4 %>% select(text) %>% distinct() %>% as.matrix()
d4_tidy <- d4_text %>% 
  str_replace_all("http.+", "") %>% 
  str_remove_all("^RT") %>% 
  str_remove_all("&amp") %>%
  str_remove_all("#.+") %>%
  as_tibble() %>% 
  unnest_tokens(word, value) %>% 
  mutate(group = "conditioner_all") %>% 
  mutate(type = "all") %>% 
  mutate(category = "conditioner")
  
 # Merge all four data frames and do the sentiment analysis
 
sentiment <- allproduct %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(group, sentiment) %>% 
  spread(key = sentiment, value = n) %>% 
  mutate(positive_over_negative = positive/negative)

sentiment %>% ggplot(aes(group, positive_over_negative)) +
            geom_bar(stat = 'identity') +
            ylab("Words of positive over negative") + xlab("Products") + ggtitle("Tweet Sentiment Analysis for Shampoos and Conditioners")
